{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark home setup in CMD before running the below code. \n",
    "\n",
    "#setx SPARK_HOME C:\\Users\\VV232LY\\spark3   \n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,FloatType,StringType\n",
    "from pyspark.sql.functions import col,round\n",
    "#from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "retail_schemas = json.load(open('C:/Users/VV232LY/Learning/pyspark/data-master/data-master/retail_db_schema.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_id', 'order_date', 'order_customer_id', 'order_status']\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "for i in retail_schemas['orders']:\n",
    "    #print(i['column_name'])\n",
    "    columns.append(i['column_name'])\n",
    "    \n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retail_schemas['orders']\n",
    "columns = list(map(lambda col: col['column_name'], retail_schemas['orders']))\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input = pd.read_csv('C:/Users/VV232LY/Learning/pyspark/data-master/data-master/retail_db/orders/part-00000', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.createDataFrame(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "df=orders.toDF(*[re.sub('[^A-Za-z0-9_]','',col) for col in orders.columns])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Daily Product Revenue \n",
    "1. One to Many relationship b/w orders and order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " order_item_id int, order_item_order_id int, order_item_product_id int, order_item_quantity int, order_item_subtotal float, order_item_product_price float\n"
     ]
    }
   ],
   "source": [
    "columns_order_items = \"\"\n",
    "schema = \"\"\n",
    "for i in retail_schemas['order_items']:\n",
    "    schema = schema +\" \"+ i['column_name'] +\" \"+ i['data_type'] + \",\"\n",
    "    \n",
    "columns_order_items = schema[:-1]\n",
    "print(columns_order_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_item_schema = StructType(\n",
    "                    [StructField(\"order_item_id\", IntegerType(), True) ,\n",
    "                     StructField(\"order_item_order_id\", IntegerType(), True) ,\n",
    "                     StructField(\"order_item_product_id\", IntegerType(), True) ,\n",
    "                     StructField(\"order_item_quantity\", IntegerType(), True) ,\n",
    "                     StructField(\"order_item_subtotal\", FloatType(), True) ,\n",
    "                     StructField(\"order_item_product_price\", FloatType(), True) \n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "#order_item_schema ='order_item_id int, order_item_order_id int,order_item_product_id int, order_item_quantity int, order_item_subtotal float,order_item_product_price float'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items= spark.read \\\n",
    "            .format('csv') \\\n",
    "            .option('sep',',') \\\n",
    "            .option('header','false') \\\n",
    "            .option('Path','C:/Users/VV232LY/Learning/pyspark/data-master/data-master/retail_db/order_items/part-00000.csv') \\\n",
    "            .schema(order_item_schema) \\\n",
    "            .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_json= spark.read \\\n",
    "            .format('json') \\\n",
    "            .option('sep',',') \\\n",
    "            .option('header','false') \\\n",
    "            .option('Path','C:/Users/VV232LY/Learning/pyspark/data-master/data-master/retail_db_json/orders/part-r-00000.json') \\\n",
    "            .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Spark Read from mysql:\n",
    "\n",
    "df=spark.read.format(\"jdbc\") \\\n",
    "             .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "             .option(\"user\",username) \\\n",
    "             .option(\"password\",password) \\\n",
    "             .option(\"url\",\"jdbc:mysql://Mysql.dbmstutorials.com:3306\") \\\n",
    "             .option(\"dbtable\",\"tutorial_db.department\") \\\n",
    "             .load()\n",
    "\n",
    "##Read with Custom query: \n",
    "\n",
    "df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "                                     user=\"tutorial_user\",\n",
    "                                     password=\"user_password\",\n",
    "                                     url=\"jdbc:mysql://Mysql.dbmstutorials.com:3306?serverTimezone=UTC&useSSL=false\",\n",
    "                                     query=\"SELECT * FROM tutorial_db.department WHERE dept_no IN (100,200)\"\n",
    "                                     ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_schema = StructType(\n",
    "                    [StructField(\"product_id\", IntegerType(), True) ,\n",
    "                     StructField(\"product_category_id\", IntegerType(), True) ,\n",
    "                     StructField(\"product_name\", StringType(), True) ,\n",
    "                     StructField(\"product_description\", StringType(), True) ,\n",
    "                     StructField(\"product_price\", FloatType(), True) ,\n",
    "                     StructField(\"product_image\", StringType(), True) \n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "\n",
    "#order_item_schema ='order_item_id int, order_item_order_id int,order_item_product_id int, order_item_quantity int, order_item_subtotal float,order_item_product_price float'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " product_id int, product_category_id int, product_name string, product_description string, product_price float, product_image string\n"
     ]
    }
   ],
   "source": [
    "columns_products = \"\"\n",
    "schema = \"\"\n",
    "for i in retail_schemas['products']:\n",
    "    schema = schema +\" \"+ i['column_name'] +\" \"+ i['data_type'] + \",\"\n",
    "    \n",
    "columns_products = schema[:-1]\n",
    "print(columns_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|         1|                  2|Quest Q64 10 FT. ...|               null|        59.98|http://images.acm...|\n",
      "|         2|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
      "|         3|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
      "|         4|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
      "|         5|                  2|Riddell Youth Rev...|               null|       199.99|http://images.acm...|\n",
      "|         6|                  2|Jordan Men's VI R...|               null|       134.99|http://images.acm...|\n",
      "|         7|                  2|Schutt Youth Recr...|               null|        99.99|http://images.acm...|\n",
      "|         8|                  2|Nike Men's Vapor ...|               null|       129.99|http://images.acm...|\n",
      "|         9|                  2|Nike Adult Vapor ...|               null|         50.0|http://images.acm...|\n",
      "|        10|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
      "|        11|                  2|Fitness Gear 300 ...|               null|       209.99|http://images.acm...|\n",
      "|        12|                  2|Under Armour Men'...|               null|       139.99|http://images.acm...|\n",
      "|        13|                  2|Under Armour Men'...|               null|        89.99|http://images.acm...|\n",
      "|        14|                  2|Quik Shade Summit...|               null|       199.99|http://images.acm...|\n",
      "|        15|                  2|Under Armour Kids...|               null|        59.99|http://images.acm...|\n",
      "|        16|                  2|Riddell Youth 360...|               null|       299.99|http://images.acm...|\n",
      "|        17|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
      "|        18|                  2|Reebok Men's Full...|               null|        29.97|http://images.acm...|\n",
      "|        19|                  2|Nike Men's Finger...|               null|       124.99|http://images.acm...|\n",
      "|        20|                  2|Under Armour Men'...|               null|       129.99|http://images.acm...|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product= spark.read \\\n",
    "            .format('csv') \\\n",
    "            .option('sep',',') \\\n",
    "            .option('header','false') \\\n",
    "            .option('Path','C:/Users/VV232LY/Learning/pyspark/data-master/data-master/retail_db/products/part-00000.csv') \\\n",
    "            .schema(columns_products) \\\n",
    "            .load()\n",
    "\n",
    "product.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+--------------------+--------------------+-------------+\n",
      "|product_category_id|product_description|product_id|       product_image|        product_name|product_price|\n",
      "+-------------------+-------------------+----------+--------------------+--------------------+-------------+\n",
      "|                  2|                   |         1|http://images.acm...|Quest Q64 10 FT. ...|        59.98|\n",
      "|                  2|                   |         2|http://images.acm...|Under Armour Men'...|       129.99|\n",
      "|                  2|                   |         3|http://images.acm...|Under Armour Men'...|        89.99|\n",
      "|                  2|                   |         4|http://images.acm...|Under Armour Men'...|        89.99|\n",
      "|                  2|                   |         5|http://images.acm...|Riddell Youth Rev...|       199.99|\n",
      "|                  2|                   |         6|http://images.acm...|Jordan Men's VI R...|       134.99|\n",
      "|                  2|                   |         7|http://images.acm...|Schutt Youth Recr...|        99.99|\n",
      "|                  2|                   |         8|http://images.acm...|Nike Men's Vapor ...|       129.99|\n",
      "|                  2|                   |         9|http://images.acm...|Nike Adult Vapor ...|         50.0|\n",
      "|                  2|                   |        10|http://images.acm...|Under Armour Men'...|       129.99|\n",
      "|                  2|                   |        11|http://images.acm...|Fitness Gear 300 ...|       209.99|\n",
      "|                  2|                   |        12|http://images.acm...|Under Armour Men'...|       139.99|\n",
      "|                  2|                   |        13|http://images.acm...|Under Armour Men'...|        89.99|\n",
      "|                  2|                   |        14|http://images.acm...|Quik Shade Summit...|       199.99|\n",
      "|                  2|                   |        15|http://images.acm...|Under Armour Kids...|        59.99|\n",
      "|                  2|                   |        16|http://images.acm...|Riddell Youth 360...|       299.99|\n",
      "|                  2|                   |        17|http://images.acm...|Under Armour Men'...|       129.99|\n",
      "|                  2|                   |        18|http://images.acm...|Reebok Men's Full...|        29.97|\n",
      "|                  2|                   |        19|http://images.acm...|Nike Men's Finger...|       124.99|\n",
      "|                  2|                   |        20|http://images.acm...|Under Armour Men'...|       129.99|\n",
      "+-------------------+-------------------+----------+--------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_json= spark.read \\\n",
    "            .format('json') \\\n",
    "            .option('sep',',') \\\n",
    "            .option('header','false') \\\n",
    "            .option('Path','C:/Users/VV232LY/Learning/pyspark/data-master/data-master/retail_db_json/products/part-r-00000') \\\n",
    "            .load()\n",
    "\n",
    "product_json.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_json.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+\n",
      "|        product_name|         avg_price|avg_price1|\n",
      "+--------------------+------------------+----------+\n",
      "|Quest Q64 10 FT. ...|59.980000000000004|     59.98|\n",
      "|Under Armour Men'...|129.98999999999995|    129.99|\n",
      "|Under Armour Men'...|             89.99|     89.99|\n",
      "|Riddell Youth Rev...|            199.99|    199.99|\n",
      "|Jordan Men's VI R...|            134.99|    134.99|\n",
      "|Schutt Youth Recr...|             99.99|     99.99|\n",
      "|Nike Men's Vapor ...|            129.99|    129.99|\n",
      "|Nike Adult Vapor ...|              50.0|      50.0|\n",
      "|Fitness Gear 300 ...|            209.99|    209.99|\n",
      "|Under Armour Men'...|            139.99|    139.99|\n",
      "|Quik Shade Summit...|            199.99|    199.99|\n",
      "|Under Armour Kids...|             59.99|     59.99|\n",
      "|Riddell Youth 360...|            299.99|    299.99|\n",
      "|Reebok Men's Full...|             29.97|     29.97|\n",
      "|Nike Men's Finger...|124.98999999999998|    124.99|\n",
      "|Under Armour Kids...|             54.99|     54.99|\n",
      "|Kijaro Dual Lock ...|29.990000000000002|     29.99|\n",
      "|Under Armour Men'...|            139.99|    139.99|\n",
      "|Elevation Trainin...|             79.99|     79.99|\n",
      "|Nike Men's USA Wh...|              90.0|      90.0|\n",
      "+--------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "select product_name,avg(product_price) as avg_price from products\n",
    "group by product_name\n",
    "\"\"\"\n",
    "#print(type(query))\n",
    "output = spark.sql(query).withColumn('avg_price1',round(col('avg_price'),2))\n",
    "\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Get number of order by date or Status\n",
    "2.Get Revenue by each order id. \n",
    "3. Get Daily product revenue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- order_item_quantity: integer (nullable = true)\n",
      " |-- order_item_subtotal: float (nullable = true)\n",
      " |-- order_item_product_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-----------+\n",
      "|order_item_order_id|revenue|avg_revenue|\n",
      "+-------------------+-------+-----------+\n",
      "|              68875|2399.95|    1199.97|\n",
      "|              68837|2299.96|    1149.98|\n",
      "|              68883|2149.99|    1074.99|\n",
      "|              68848|2399.96|     799.99|\n",
      "|              68859|2349.89|      783.3|\n",
      "|              68736|2259.95|     753.32|\n",
      "|              68722|2199.99|     733.33|\n",
      "|              68822|1399.95|     699.97|\n",
      "|              68809|2779.86|     694.97|\n",
      "|              68703|3449.91|     689.98|\n",
      "|              68806|2629.92|     657.48|\n",
      "|              68821|2629.92|     657.48|\n",
      "|              68723|1239.95|     619.97|\n",
      "|              68811| 599.99|     599.99|\n",
      "|              68724|2859.89|     571.98|\n",
      "|              68858|2839.91|     567.98|\n",
      "|              68766| 2699.9|     539.98|\n",
      "|              68778| 2629.9|     525.98|\n",
      "|              61904|  500.0|      500.0|\n",
      "|              68748| 999.97|     499.99|\n",
      "+-------------------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame[order_item_order_id: int, revenue: double, avg_revenue: double]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "subtotal = order_items.groupBy('order_item_order_id') \\\n",
    "                      .agg(F.round(F.sum('order_item_subtotal'),2).alias('revenue'),\n",
    "                                   F.round(F.avg('order_item_subtotal'),2).alias('avg_revenue')\n",
    "                          ) \\\n",
    "                     .orderBy(col('avg_revenue').desc(),col('revenue').desc())\n",
    "                    \n",
    "subtotal.show()\n",
    "print(subtotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|profit|cnd_profit|\n",
      "+------+----------+\n",
      "|     Y|     25283|\n",
      "|     N|     32148|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task 1: Total Number of Records: \n",
    "#order_items.show()\n",
    "#print(order_items.count())\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "grouped_order_item= order_items.groupBy(F.col('order_item_order_id')) \\\n",
    "                                .agg(F.round(F.sum(F.col('order_item_subtotal')),2).alias('total_amount_per_order'),\n",
    "                                    F.round(F.avg(F.col('order_item_subtotal')),2).alias('avg_amount_per_order'),\n",
    "                                     F.max(F.col('order_item_quantity')).alias('max_qty'),\n",
    "                                     F.min(F.col('order_item_quantity')).alias('min_qty'),\n",
    "                                    ) \\\n",
    "                                .orderBy(F.col('total_amount_per_order').desc(),F.col('avg_amount_per_order')) \\\n",
    "                                .withColumn('profit',F.expr(\"case when avg_amount_per_order >= 200 then 'Y' else 'N' end\")) \\\n",
    "                                .groupBy(F.col('profit')).agg(F.count(F.col('profit')).alias('cnd_profit'))\n",
    "                            \n",
    "\n",
    "    \n",
    "#grouped_order_item.grou\n",
    "# print(type(grouped_order_item))\n",
    " \n",
    "#grouped_order_item.show()\n",
    "\n",
    "\n",
    "#new_df = grouped_order_item.\n",
    "\n",
    "\n",
    "# new_order_item_df = order_items.filter(F.col('order_item_order_id').isNotNull())\n",
    "\n",
    "# print(new_order_item_df.count())  300\n",
    "# print(order_items.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|\n",
      "|           11|                  5|                 1014|                  2|              99.96|                   49.98|\n",
      "|           12|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           13|                  5|                  403|                  1|             129.99|                  129.99|\n",
      "|           14|                  7|                 1073|                  1|             199.99|                  199.99|\n",
      "|           15|                  7|                  957|                  1|             299.98|                  299.98|\n",
      "|           16|                  7|                  926|                  5|              79.95|                   15.99|\n",
      "|           17|                  8|                  365|                  3|             179.97|                   59.99|\n",
      "|           18|                  8|                  365|                  5|             299.95|                   59.99|\n",
      "|           19|                  8|                 1014|                  4|             199.92|                   49.98|\n",
      "|           20|                  8|                  502|                  1|               50.0|                    50.0|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|order_item_id|avg_amt_per_order|\n",
      "+-------------+-----------------+\n",
      "|          148|           399.96|\n",
      "|          463|           399.98|\n",
      "|          471|            150.0|\n",
      "|          496|            59.99|\n",
      "|          833|           399.98|\n",
      "|         1088|           299.95|\n",
      "|         1238|            59.99|\n",
      "|         1342|           239.96|\n",
      "|         1580|           199.99|\n",
      "|         1591|           199.99|\n",
      "|         1645|           399.98|\n",
      "|         1829|           399.98|\n",
      "|         1959|           129.99|\n",
      "|         2122|            59.97|\n",
      "|         2142|            79.98|\n",
      "|         2366|           199.98|\n",
      "|         2659|           299.98|\n",
      "|         2866|             50.0|\n",
      "|         3175|           129.99|\n",
      "|         3749|           399.98|\n",
      "+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 2: Avg order amount per order.\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "avg_order_amt = order_items.groupBy(col('order_item_id')).agg(F.round(F.avg(F.col('order_item_subtotal')),2).alias('avg_amt_per_order'))\n",
    "\n",
    "avg_order_amt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------+---------------+------------------+\n",
      "|order_item_order_id|min_amt_per_order|max_amt_per_order|number_of_items|total_orders_value|\n",
      "+-------------------+-----------------+-----------------+---------------+------------------+\n",
      "|              68724|            59.99|          1999.99|              5|            2860.0|\n",
      "|              68778|           129.99|          1999.99|              5|            2630.0|\n",
      "|              68766|            49.98|          1999.99|              5|            2700.0|\n",
      "|              68858|            100.0|          1999.99|              5|            2840.0|\n",
      "|              68703|           199.95|          1999.99|              5|            3450.0|\n",
      "|              68780|            99.96|           999.99|              5|            1730.0|\n",
      "|              68786|            59.99|           999.99|              5|            1590.0|\n",
      "|              68733|            59.99|           599.99|              5|            1360.0|\n",
      "|              68710|            100.0|           599.99|              5|            1500.0|\n",
      "|              68843|           179.97|           599.99|              5|            1760.0|\n",
      "|              68862|            59.99|           599.99|              5|            1660.0|\n",
      "|              68773|            79.98|           599.99|              5|            1330.0|\n",
      "|              68770|           129.99|           599.99|              5|            1480.0|\n",
      "|              68816|           129.99|           599.99|              5|            2330.0|\n",
      "|              62542|           129.99|            500.0|              5|            1090.0|\n",
      "|              61838|            99.99|            500.0|              5|            1230.0|\n",
      "|              63452|            99.96|            500.0|              5|            1430.0|\n",
      "|              58921|           119.98|            500.0|              5|            1620.0|\n",
      "|              66597|             50.0|            500.0|              5|            1550.0|\n",
      "|              61482|            49.98|            500.0|              5|            1100.0|\n",
      "+-------------------+-----------------+-----------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 2: Max and Min order order_item_quantity .\n",
    "\n",
    "min_max_order_amt = order_items.groupBy(col('order_item_order_id')) \\\n",
    "                           .agg(F.round(F.min(F.col('order_item_subtotal')),2).alias('min_amt_per_order'),\n",
    "                                F.round(F.max(F.col('order_item_subtotal')),2).alias('max_amt_per_order'),\n",
    "                                F.count(F.col('order_item_subtotal')).alias('number_of_items'),\n",
    "                                F.round(F.sum(F.col('order_item_subtotal'))).alias('total_orders_value')\n",
    "                               ) \\\n",
    "                            .filter(F.col('number_of_items') > 1) \\\n",
    "                            .orderBy(F.col('number_of_items').desc(),\n",
    "                                     F.col('max_amt_per_order').desc())\n",
    "\n",
    "min_max_order_amt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1315.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 171.0 failed 1 times, most recent failure: Lost task 0.0 in stage 171.0 (TID 158) (IN3017677W1.ey.net executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:293)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:173)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:167)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:143)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:176)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:258)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:256)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:256)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:228)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:367)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:340)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-50c5437cf3a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_max_order_amt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rank'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_total_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1315.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 171.0 failed 1 times, most recent failure: Lost task 0.0 in stage 171.0 (TID 158) (IN3017677W1.ey.net executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:293)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:173)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:167)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:143)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:174)\r\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:176)\r\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:258)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:256)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:256)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:228)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:367)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:340)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n"
     ]
    }
   ],
   "source": [
    "#Task 3: Window Aggregate: Duplicate removal\n",
    "\n",
    "from pyspark.sql import Window\n",
    "\n",
    "running_total_window = Window.partitionBy('order_item_order_id') \\\n",
    "                            .orderBy('total_orders_value') \\\n",
    "                            .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "\n",
    "new_df = min_max_order_amt.withColumn('rank',F.row_number().over(running_total_window))\n",
    "\n",
    "new_df.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
